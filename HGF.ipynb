{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cypher29/AI_repo/blob/main/HGF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gowivd2dKLLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f606d5c-4a45-425a-f7c3-1cdea9475bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv\n",
        "#pip install anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nTFfMEZuLFX3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JzwAzUQkKLgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOaR5qn9h-qg"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8o11OFXA_fx"
      },
      "outputs": [],
      "source": [
        "# if this gives an \"ERROR\" about pip dependency conflicts, ignore it! It doesn't affect anything.\n",
        "!pip install -q transformers datasets diffusers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio is for quick User Inerface design"
      ],
      "metadata": {
        "id": "XXd8V6-rKokN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4NwZDI3KADPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82960c75-c461-48e8-cd1e-0b1e5cf58d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.8 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ueABIFWsTgru"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline\n",
        "from diffusers import DiffusionPipeline\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio\n",
        "import gradio as gr\n",
        "import os\n",
        "#import anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wk7PuP7JVO0A"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get('HUGGINGFACE_KEY')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyyNbTcCD8Qf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fsamHXaWVby5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7328ffe-c1e3-4ddc-c808-aa4a21e02260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9995296001434326}]\n"
          ]
        }
      ],
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "#classifier = pipeline(\"sentiment-analysis\", device=\"cuda\")\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"LLM's are very helpful\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcl0T4Mbsv_v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "inputs = tokenizer(\"Welcome to mysteries of LLM\", return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "len(inputs)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "print(logits)\n",
        "predicted_class_id = logits.argmax().item()\n",
        "print(predicted_class_id)\n",
        "model.config.id2label[predicted_class_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtuEe4i5uA4c"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english', trust_remote_code=True)\n",
        "text = \"Welcome to mysteries of LLM\"\n",
        "tokens = tokenizer.encode(text)\n",
        "print(tokens)\n",
        "tokenizer.decode(tokens)\n",
        "tokenizer.batch_decode(tokens)\n",
        "tokenizer.get_added_vocab()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02aADYhC7Sld"
      },
      "outputs": [],
      "source": [
        "from google import genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4oOQWVp7YY7"
      },
      "outputs": [],
      "source": [
        "GOOGLE_token = userdata.get('GOOGLE_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0sD0Stc7rCy"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\", contents=\"Hi there\"\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6Stq9xb_03P"
      },
      "outputs": [],
      "source": [
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypmOU7gpAO3P"
      },
      "outputs": [],
      "source": [
        "def greet(name, intensity):\n",
        "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=greet,\n",
        "    inputs=[\"text\", \"slider\"],\n",
        "    outputs=[\"text\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzAaRj-uAYx4"
      },
      "outputs": [],
      "source": [
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnnycjsaXBMU"
      },
      "outputs": [],
      "source": [
        "# Named Entity Recognition\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True, device=\"cuda\")\n",
        "result = ner(\"Modi is the prime minister of India\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffXeiLboXZA_"
      },
      "outputs": [],
      "source": [
        "# Question Answering with Context\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", device=\"cuda\")\n",
        "result = question_answerer(question=\"Who is prime minister of India?\", context=\"Modi is the prime minister of India. He is from BJP government\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhwxuISxYFSN"
      },
      "outputs": [],
      "source": [
        "# Text Summarization\n",
        "\n",
        "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
        "text = \"\"\"The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
        "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
        "It's an extremely popular library that's widely used by the open-source data science community.\n",
        "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.\n",
        "\"\"\"\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQSjh1DkYcK6"
      },
      "outputs": [],
      "source": [
        "# Translation\n",
        "\n",
        "translator = pipeline(\"translation_en_to_fr\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X97StA56Ynu9"
      },
      "outputs": [],
      "source": [
        "# Another translation, showing a model being specified\n",
        "# All translation models are here: https://huggingface.co/models?pipeline_tag=translation&sort=trending\n",
        "\n",
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzMy2E0_YyQR"
      },
      "outputs": [],
      "source": [
        "# Classification\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", device=\"cuda\")\n",
        "result = classifier(\"Hugging Face's Transformers library is amazing!\", candidate_labels=[\"technology\", \"sports\", \"politics\"])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgwOM5BYZCdm"
      },
      "outputs": [],
      "source": [
        "# Text Generation\n",
        "\n",
        "generator = pipeline(\"text-generation\", device=\"cuda\")\n",
        "result = generator(\"In bodybuilding biceps training is easy also abs training\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkF2c8ZfZrWo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_kFN1e_ZcnI"
      },
      "outputs": [],
      "source": [
        "# Image Generation\n",
        "\n",
        "image_gen = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "text = \"a pluffy smily wolf which is happy\"\n",
        "image = image_gen(prompt=text).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOzBKvZzabNe"
      },
      "outputs": [],
      "source": [
        "# Audio Generation\n",
        "\n",
        "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cuda')\n",
        "\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "speech = synthesiser(\"Hi daksha, how are you today. \", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "\n",
        "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n",
        "Audio(\"speech.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LRiSIyUqWCC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
        "model.to(\"cuda\")\n",
        "\n",
        "generated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n",
        "\n",
        "# decode with mistral tokenizer\n",
        "result = tokenizer.decode(generated_ids[0].tolist())\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGqWvLaGeIO7"
      },
      "source": [
        "Open AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RWnEfXmFeHVK"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_KEY')\n",
        "MODEL = 'gpt-4o-mini'\n",
        "openai = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7h65pBSiDG8"
      },
      "outputs": [],
      "source": [
        "system_message = \"You are an assistant that is great at telling kids stories\"\n",
        "user_prompt = \"Tell a small story about animals in forrest\"\n",
        "\n",
        "prompts = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da0dZDIqiJ0z"
      },
      "outputs": [],
      "source": [
        "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyZMyZmPiWQk"
      },
      "outputs": [],
      "source": [
        "completion = openai.chat.completions.create(\n",
        "    model='gpt-4o',\n",
        "    messages=prompts,\n",
        "    temperature=0.4\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZeuLXpOin9c"
      },
      "outputs": [],
      "source": [
        "len(completion.choices)\n",
        "print(completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcDOGSkQkB1f"
      },
      "outputs": [],
      "source": [
        "def mychat(text):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": text}\n",
        "      ]\n",
        "    completion  = openai.chat.completions.create(\n",
        "    model='gpt-4o',\n",
        "    messages=messages,\n",
        "    temperature=0.4\n",
        "    )\n",
        "    print(completion)\n",
        " #   result = \"\"\n",
        " #   for chunk in stream:\n",
        " #      result += chunk.choices[0].delta.content or \"\"\n",
        " #      print(result)\n",
        " #      yield result\n",
        "    return completion.choices[0].message.content\n",
        "#gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\").launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr6WZKKynbaZ"
      },
      "outputs": [],
      "source": [
        "mychat('hi there',[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoE3BveHklYH"
      },
      "outputs": [],
      "source": [
        "system_message = \"You are a helpful assistant that responds in markdown\"\n",
        "view = gr.Interface(\n",
        "    fn=mychat,\n",
        "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
        "    outputs=[gr.Textbox(label=\"Response:\", lines=6)],\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "view.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBJ4yIQ7rquX"
      },
      "outputs": [],
      "source": [
        "def mychat2(message, history):\n",
        "    #pdb.set_trace()\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    print(\"History is:\")\n",
        "    print(history)\n",
        "    print(\"And messages is:\")\n",
        "    print(messages)\n",
        "\n",
        "    completion  = openai.chat.completions.create(\n",
        "    model='gpt-4o',\n",
        "    messages=messages,\n",
        "    temperature=0.4\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are a helpful assistant that responds in markdown\"\n",
        "gr.ChatInterface(fn=mychat2, type=\"messages\").launch()"
      ],
      "metadata": {
        "id": "cmsi2T8QRe4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiUQPy-pDKDe"
      },
      "source": [
        "**Agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "S7FVkD-lsXmE"
      },
      "outputs": [],
      "source": [
        "system_message = \"You are a helpful assistant for a bookshop in India. \"\n",
        "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
        "system_message += \"Always be accurate. If you don't know the answer, say so.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book_prices = {\"Panchatantra\": \"450\", \"Prince\": \"999\", \"The Hobbit\": \"750\", \"Dune\": \"500\"}"
      ],
      "metadata": {
        "id": "WwsxSL7xO9EN"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#book_prices[\"Tenali\"] = \"1240\"\n",
        "book_prices = {**book_prices,  \"The Arrival\": \"2500\", 'Goodnight Moon': '475'}"
      ],
      "metadata": {
        "id": "5IESm4LuQLiO"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "VyRdeex-tmur"
      },
      "outputs": [],
      "source": [
        "def get_book_price(bookName):\n",
        "    #pdb.set_trace()\n",
        "    print(f\"Tool get_book_price called for {bookName}\")\n",
        "    book = bookName.lower()\n",
        "    for k,v in book_prices.items():\n",
        "      if(k.lower() == book):\n",
        "        return v\n",
        "    return \"Unknown\"\n",
        "    #return book_prices.get(book, \"Unknown\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "S9M7NqBOHwDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94207f55-278e-4942-bb52-1275fe672d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'two cities': '450', 'prince': '999', 'the hobbit': '750', 'dune': '500', 'The Arrival': '2500', 'Goodnight Moon': '475'}\n"
          ]
        }
      ],
      "source": [
        "get_book_price(\"The Arrival\")\n",
        "\n",
        "print(book_prices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "I-0I8DXSt1Kz"
      },
      "outputs": [],
      "source": [
        "price_function = {\n",
        "    \"name\": \"get_book_price\",\n",
        "    \"description\": \"Get the price of a book. Call this whenever you need to know the book price, for example when a customer asks 'How much is this book'\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"bookName\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The city that the customer wants to travel to\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"bookName\"],\n",
        "        \"additionalProperties\": False\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oehdO1UUurxm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "LHj4pvxquCtq"
      },
      "outputs": [],
      "source": [
        "tools = [{\"type\": \"function\", \"function\": price_function}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "zgUMmjlkucfb"
      },
      "outputs": [],
      "source": [
        "import pdb\n",
        "def chat(message, history):\n",
        "  #  if(history == None):\n",
        "   # history = [{'role': 'user', 'metadata': None, 'content': 'hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'yes', 'options': None}]\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
        "    #pdb.set_trace()\n",
        "    if response.choices[0].finish_reason==\"tool_calls\":\n",
        "        message = response.choices[0].message\n",
        "        response, city = handle_tool_call(message)\n",
        "        messages.append(message)\n",
        "        messages.append(response)\n",
        "        response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
        "    #pdb.set_trace()\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "UL_7bLbJu1x9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def handle_tool_call(message):\n",
        "    tool_call = message.tool_calls[0]\n",
        "    arguments = json.loads(tool_call.function.arguments)\n",
        "    book = arguments.get('bookName')\n",
        "    price = get_book_price(book)\n",
        "    response = {\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": json.dumps({\"bookName\": book,\"price\": price}),\n",
        "        \"tool_call_id\": tool_call.id\n",
        "    }\n",
        "    return response, price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwxcBty9wmAZ"
      },
      "outputs": [],
      "source": [
        "chat('Prince', [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-o8Zpsxrsvt"
      },
      "outputs": [],
      "source": [
        "gr.ChatInterface(fn=chat, type=\"messages\").launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demo of Gradio"
      ],
      "metadata": {
        "id": "RPiUopQJPQGL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Np4ldloAnch"
      },
      "outputs": [],
      "source": [
        "\n",
        "def echo(message, history):\n",
        "    if(history == None):\n",
        "        history = []\n",
        "\n",
        "    return message+\" \"+str(history)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gHWW2OpBtYr"
      },
      "outputs": [],
      "source": [
        "demo = gr.ChatInterface(fn=echo, type=\"messages\", examples=[\"welcome\", \"hellow\", \"what's up\"], title=\"Echo Bot\")\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using **Claude**"
      ],
      "metadata": {
        "id": "jPMwUosPSFuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "claude_key = userdata.get('CLAUDE_KEY')\n",
        "claude = anthropic.Anthropic(\n",
        "    api_key=claude_key,\n",
        ")\n",
        "#claude = anthropic.Anthropic()\n",
        "system_message = \"You are a helpful assistant\""
      ],
      "metadata": {
        "id": "Htgk0AVdCzPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_claude(prompt):\n",
        "    result = claude.messages.create(\n",
        "        model=\"claude-3-haiku-20240307\",\n",
        "        max_tokens=1000,\n",
        "        temperature=0.7,\n",
        "        system=system_message,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    return result.completion"
      ],
      "metadata": {
        "id": "aQ223MpTEJuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stream_claude(\"Hi there\"))\n"
      ],
      "metadata": {
        "id": "JCAgJbQOET4B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1J_jGRq73--B7dePZwPlyKEp9nKf8zMl2",
      "authorship_tag": "ABX9TyM3RLXgtlXuG9SWog5QCUb3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}