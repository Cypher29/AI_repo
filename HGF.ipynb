{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cypher29/AI_repo/blob/main/HGF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gowivd2dKLLN"
      },
      "outputs": [],
      "source": [
        "pip install python-dotenv\n",
        "#pip install anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nTFfMEZuLFX3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JzwAzUQkKLgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOaR5qn9h-qg"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8o11OFXA_fx"
      },
      "outputs": [],
      "source": [
        "# if this gives an \"ERROR\" about pip dependency conflicts, ignore it! It doesn't affect anything.\n",
        "!pip install -q transformers datasets diffusers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio is for quick User Inerface design"
      ],
      "metadata": {
        "id": "XXd8V6-rKokN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4NwZDI3KADPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c014097-ef03-46e2-aaf7-9caebaf1f39a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.8 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ueABIFWsTgru"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline\n",
        "from diffusers import DiffusionPipeline\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio\n",
        "import gradio as gr\n",
        "import os\n",
        "#import anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wk7PuP7JVO0A"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get('HUGGINGFACE_KEY')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyyNbTcCD8Qf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fsamHXaWVby5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8926ce4c-325d-4061-de0d-7285fda209d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9995296001434326}]\n"
          ]
        }
      ],
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "#classifier = pipeline(\"sentiment-analysis\", device=\"cuda\")\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"LLM's are very helpful\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcl0T4Mbsv_v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "inputs = tokenizer(\"Welcome to mysteries of LLM\", return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "len(inputs)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "print(logits)\n",
        "predicted_class_id = logits.argmax().item()\n",
        "print(predicted_class_id)\n",
        "model.config.id2label[predicted_class_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtuEe4i5uA4c"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english', trust_remote_code=True)\n",
        "text = \"Welcome to mysteries of LLM\"\n",
        "tokens = tokenizer.encode(text)\n",
        "print(tokens)\n",
        "tokenizer.decode(tokens)\n",
        "tokenizer.batch_decode(tokens)\n",
        "tokenizer.get_added_vocab()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02aADYhC7Sld"
      },
      "outputs": [],
      "source": [
        "from google import genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4oOQWVp7YY7"
      },
      "outputs": [],
      "source": [
        "GOOGLE_token = userdata.get('GOOGLE_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0sD0Stc7rCy"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\", contents=\"Hi there\"\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6Stq9xb_03P"
      },
      "outputs": [],
      "source": [
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypmOU7gpAO3P"
      },
      "outputs": [],
      "source": [
        "def greet(name, intensity):\n",
        "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=greet,\n",
        "    inputs=[\"text\", \"slider\"],\n",
        "    outputs=[\"text\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzAaRj-uAYx4"
      },
      "outputs": [],
      "source": [
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnnycjsaXBMU"
      },
      "outputs": [],
      "source": [
        "# Named Entity Recognition\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True, device=\"cuda\")\n",
        "result = ner(\"Modi is the prime minister of India\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffXeiLboXZA_"
      },
      "outputs": [],
      "source": [
        "# Question Answering with Context\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", device=\"cuda\")\n",
        "result = question_answerer(question=\"Who is prime minister of India?\", context=\"Modi is the prime minister of India. He is from BJP government\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhwxuISxYFSN"
      },
      "outputs": [],
      "source": [
        "# Text Summarization\n",
        "\n",
        "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
        "text = \"\"\"The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
        "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
        "It's an extremely popular library that's widely used by the open-source data science community.\n",
        "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.\n",
        "\"\"\"\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQSjh1DkYcK6"
      },
      "outputs": [],
      "source": [
        "# Translation\n",
        "\n",
        "translator = pipeline(\"translation_en_to_fr\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X97StA56Ynu9"
      },
      "outputs": [],
      "source": [
        "# Another translation, showing a model being specified\n",
        "# All translation models are here: https://huggingface.co/models?pipeline_tag=translation&sort=trending\n",
        "\n",
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzMy2E0_YyQR"
      },
      "outputs": [],
      "source": [
        "# Classification\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", device=\"cuda\")\n",
        "result = classifier(\"Hugging Face's Transformers library is amazing!\", candidate_labels=[\"technology\", \"sports\", \"politics\"])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgwOM5BYZCdm"
      },
      "outputs": [],
      "source": [
        "# Text Generation\n",
        "\n",
        "generator = pipeline(\"text-generation\", device=\"cuda\")\n",
        "result = generator(\"In bodybuilding biceps training is easy also abs training\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkF2c8ZfZrWo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_kFN1e_ZcnI"
      },
      "outputs": [],
      "source": [
        "# Image Generation\n",
        "\n",
        "image_gen = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "text = \"a pluffy smily wolf which is happy\"\n",
        "image = image_gen(prompt=text).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOzBKvZzabNe"
      },
      "outputs": [],
      "source": [
        "# Audio Generation\n",
        "\n",
        "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cuda')\n",
        "\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "speech = synthesiser(\"Hi daksha, how are you today. \", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "\n",
        "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n",
        "Audio(\"speech.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LRiSIyUqWCC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
        "model.to(\"cuda\")\n",
        "\n",
        "generated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n",
        "\n",
        "# decode with mistral tokenizer\n",
        "result = tokenizer.decode(generated_ids[0].tolist())\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGqWvLaGeIO7"
      },
      "source": [
        "Open AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RWnEfXmFeHVK"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_KEY')\n",
        "MODEL = 'gpt-4o-mini'\n",
        "openai = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I7h65pBSiDG8"
      },
      "outputs": [],
      "source": [
        "system_message = \"You are an assistant that is great at telling kids stories\"\n",
        "user_prompt = \"Tell a small story about animals in forrest\"\n",
        "\n",
        "prompts = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da0dZDIqiJ0z"
      },
      "outputs": [],
      "source": [
        "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyZMyZmPiWQk"
      },
      "outputs": [],
      "source": [
        "completion = openai.chat.completions.create(\n",
        "    model='gpt-4o',\n",
        "    messages=prompts,\n",
        "    temperature=0.4\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZeuLXpOin9c"
      },
      "outputs": [],
      "source": [
        "len(completion.choices)\n",
        "print(completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HcDOGSkQkB1f"
      },
      "outputs": [],
      "source": [
        "def mychat(text):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": text}\n",
        "      ]\n",
        "    completion  = openai.chat.completions.create(\n",
        "    model='gpt-4o',\n",
        "    messages=messages,\n",
        "    temperature=0.4\n",
        "    )\n",
        "    print(completion)\n",
        " #   result = \"\"\n",
        " #   for chunk in stream:\n",
        " #      result += chunk.choices[0].delta.content or \"\"\n",
        " #      print(result)\n",
        " #      yield result\n",
        "    return completion.choices[0].message.content\n",
        "#gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\").launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wr6WZKKynbaZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1f09dc47-104a-4f3e-b92f-61689634c3ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-BU1HAcFrPec4Gk0O4fv9ozOTMpVpQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today? Would you like to hear a story or perhaps create one together?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746494444, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_d8864f8b6b', usage=CompletionUsage(completion_tokens=23, prompt_tokens=24, total_tokens=47, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today? Would you like to hear a story or perhaps create one together?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "mychat('hi there')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xoE3BveHklYH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "431d4c83-84b9-4053-b991-b23af790e808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://caf536c52d91884546.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://caf536c52d91884546.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "system_message = \"You are a helpful assistant that responds in markdown\"\n",
        "view = gr.Interface(\n",
        "    fn=mychat,\n",
        "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
        "    outputs=[gr.Textbox(label=\"Response:\", lines=6)],\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "view.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WBJ4yIQ7rquX"
      },
      "outputs": [],
      "source": [
        "def chat(message, history):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    print(\"History is:\")\n",
        "    print(history)\n",
        "    print(\"And messages is:\")\n",
        "    print(messages)\n",
        "\n",
        "    completion  = openai.chat.completions.create(\n",
        "    model='gpt-4o',\n",
        "    messages=messages,\n",
        "    temperature=0.4\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiUQPy-pDKDe"
      },
      "source": [
        "**Agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "S7FVkD-lsXmE"
      },
      "outputs": [],
      "source": [
        "system_message = \"You are a helpful assistant for a bookshop. \"\n",
        "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
        "system_message += \"Always be accurate. If you don't know the answer, say so.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VyRdeex-tmur"
      },
      "outputs": [],
      "source": [
        "book_prices = {\"Two Cities\": \"450\", \"Prince\": \"999\", \"The Hobbit\": \"750\", \"Dune\": \"500\"}\n",
        "\n",
        "def get_book_price(bookName):\n",
        "    #pdb.set_trace()\n",
        "    print(f\"Tool get_book_price called for {bookName}\")\n",
        "    book = bookName.lower()\n",
        "    return book_prices.get(book, \"Unknown\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9M7NqBOHwDg"
      },
      "outputs": [],
      "source": [
        "get_ticket_price(\"London\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "I-0I8DXSt1Kz"
      },
      "outputs": [],
      "source": [
        "price_function = {\n",
        "    \"name\": \"get_book_price\",\n",
        "    \"description\": \"Get the price of a book. Call this whenever you need to know the book price, for example when a customer asks 'How much is this book'\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"bookName\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The city that the customer wants to travel to\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"bookName\"],\n",
        "        \"additionalProperties\": False\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oehdO1UUurxm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LHj4pvxquCtq"
      },
      "outputs": [],
      "source": [
        "tools = [{\"type\": \"function\", \"function\": price_function}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zgUMmjlkucfb"
      },
      "outputs": [],
      "source": [
        "import pdb\n",
        "def chat(message, history):\n",
        "  #  if(history == None):\n",
        "   # history = [{'role': 'user', 'metadata': None, 'content': 'hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'yes', 'options': None}]\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
        "    #pdb.set_trace()\n",
        "    if response.choices[0].finish_reason==\"tool_calls\":\n",
        "        message = response.choices[0].message\n",
        "        response, city = handle_tool_call(message)\n",
        "        messages.append(message)\n",
        "        messages.append(response)\n",
        "        response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
        "    #pdb.set_trace()\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UL_7bLbJu1x9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def handle_tool_call(message):\n",
        "    tool_call = message.tool_calls[0]\n",
        "    arguments = json.loads(tool_call.function.arguments)\n",
        "    book = arguments.get('destination_city')\n",
        "    price = get_book_price(book)\n",
        "    response = {\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": json.dumps({\"bookName\": book,\"price\": price}),\n",
        "        \"tool_call_id\": tool_call.id\n",
        "    }\n",
        "    return response, city"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OwxcBty9wmAZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "b6ce9ee7-a5aa-4e26-96d8-e81bbffc78b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool get_book_price called for None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8fc55b09b095>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prince'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-1116410e3f25>\u001b[0m in \u001b[0;36mchat\u001b[0;34m(message, history)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish_reason\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"tool_calls\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_tool_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-4b458858be63>\u001b[0m in \u001b[0;36mhandle_tool_call\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'destination_city'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_book_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     response = {\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"tool\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-0fd36d527cc4>\u001b[0m in \u001b[0;36mget_book_price\u001b[0;34m(bookName)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Tool get_book_price called for {bookName}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbookName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mticket_prices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Unknown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
          ]
        }
      ],
      "source": [
        "chat('Prince', [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-o8Zpsxrsvt"
      },
      "outputs": [],
      "source": [
        "gr.ChatInterface(fn=chat, type=\"messages\").launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Np4ldloAnch"
      },
      "outputs": [],
      "source": [
        "\n",
        "def echo(message, history):\n",
        "    if(history == None):\n",
        "        history = []\n",
        "\n",
        "    return message+\" \"+str(history)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gHWW2OpBtYr"
      },
      "outputs": [],
      "source": [
        "demo = gr.ChatInterface(fn=echo, type=\"messages\", examples=[\"hello\", \"hola\", \"merhaba\"], title=\"Echo Bot\")\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using **Claude**"
      ],
      "metadata": {
        "id": "jPMwUosPSFuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "claude_key = userdata.get('CLAUDE_KEY')\n",
        "claude = anthropic.Anthropic(\n",
        "    api_key=claude_key,\n",
        ")\n",
        "#claude = anthropic.Anthropic()\n",
        "system_message = \"You are a helpful assistant\""
      ],
      "metadata": {
        "id": "Htgk0AVdCzPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_claude(prompt):\n",
        "    result = claude.messages.create(\n",
        "        model=\"claude-3-haiku-20240307\",\n",
        "        max_tokens=1000,\n",
        "        temperature=0.7,\n",
        "        system=system_message,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    return result.completion"
      ],
      "metadata": {
        "id": "aQ223MpTEJuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stream_claude(\"Hi there\"))\n"
      ],
      "metadata": {
        "id": "JCAgJbQOET4B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1J_jGRq73--B7dePZwPlyKEp9nKf8zMl2",
      "authorship_tag": "ABX9TyN9/9jvE+6Ss/dA6aPeqMZx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}