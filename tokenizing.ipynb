{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgiFwDxb9M59bYOraOLIcI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cypher29/AI_repo/blob/main/tokenizing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "7dCWrcV9wk3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel"
      ],
      "metadata": {
        "id": "QlEACyLpv8X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGdPwnoPCJrT"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import pipeline\n",
        "#import anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HUGGINGFACE_KEY')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "CaEuVaCVDgmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoModel.from_pretrained('google-bert/bert-base-uncased')"
      ],
      "metadata": {
        "id": "eqdsjcs4xuT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_9xpPBRB2AA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(config)"
      ],
      "metadata": {
        "id": "FLOs0Ag6x4uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "QWEN2_MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
        "STARCODER2_MODEL_NAME = \"bigcode/starcoder2-3b\""
      ],
      "metadata": {
        "id": "kwTuF6awGLHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "tokens = phi3_tokenizer.encode(text)\n",
        "print(phi3_tokenizer.batch_decode(tokens))"
      ],
      "metadata": {
        "id": "90Oax49OGQpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qwen2_tokenizer = AutoTokenizer.from_pretrained(QWEN2_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "print(phi3_tokenizer.encode(text))\n",
        "print()\n",
        "print(qwen2_tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "R4gxqjF_GRiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wjnKIFAwj2OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is sentiment analysis model"
      ],
      "metadata": {
        "id": "-GaXguRBFdaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased-finetuned-sst-2-english', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "WAepaFSyD7Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "qQ_81rDEkjBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer)"
      ],
      "metadata": {
        "id": "4OdFUN_mkDnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "bMdL7IagmB5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, Explain about software development life cycle\"\n",
        "tokens = tokenizer.encode(text)\n",
        "vocabsize = tokenizer.vocab_size\n",
        "print(\"Vocab size : \" ,vocabsize)\n",
        "tokens"
      ],
      "metadata": {
        "id": "s9HQrJJMEU2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "JWzxPub_Ee5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokens)"
      ],
      "metadata": {
        "id": "oW4-suVSEkZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.all_special_tokens"
      ],
      "metadata": {
        "id": "Rcx1dbd5lMtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(tokens)"
      ],
      "metadata": {
        "id": "NApVc6TpEoom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_added_vocab()"
      ],
      "metadata": {
        "id": "PvwdQwoTEucl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\""
      ],
      "metadata": {
        "id": "sDbQV0ZZFawn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "tokens = phi3_tokenizer.encode(text)\n",
        "print(phi3_tokenizer.batch_decode(tokens))"
      ],
      "metadata": {
        "id": "n7N02WR-FlZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.all_special_tokens\n",
        "tokenizer.get_vocab()\n"
      ],
      "metadata": {
        "id": "RGOP6f4xX8tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "inputs = tokenizer(\"My bike has trouble and I feel good as I don't need to got to market \", return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "len(inputs)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "print(logits)\n",
        "predicted_class_id = logits.argmax().item()\n",
        "print(predicted_class_id)\n",
        "model.config.id2label[predicted_class_id]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "svLmVjDolatV",
        "outputId": "4589a5ac-2b44-4b94-9fa7-5eab6125a62d"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 2026, 7997, 2038, 4390, 1998, 1045, 2514, 2204, 2004, 1045, 2123,\n",
            "         1005, 1056, 2342, 2000, 2288, 2000, 3006,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "tensor([[-4.1062,  4.4022]])\n",
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'POSITIVE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qlTIiUSmt7Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWnEfXmFeHVK"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_KEY')\n",
        "MODEL = 'gpt-4o-mini'\n",
        "openai = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant that is great at completing sentense. Always be accurate. If you don't know the answer, say so.\"\n",
        "#user_prompt = \"Plucking apples from apple tree in apestreat. How many p's are there in above sentense\"\n",
        "user_prompt = \"food ball is a game\"\n",
        "prompts = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]"
      ],
      "metadata": {
        "id": "CHKEyhqRuHqj"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gpt-3.5-turbo\n",
        "#gpt-4.1\n",
        "completion = openai.chat.completions.create(model='gpt-4.1', messages=prompts, temperature=0.8)\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxNDRorhwgDe",
        "outputId": "a5c1e350-9571-4446-f1ef-f0e2b451a494"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Football is a game played between two teams, where the objective is to score points by getting the ball into the opposing team's goal or end zone, depending on the type of football (such as soccer, American football, or rugby).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
        "Response = openai.responses.create(model='gpt-4.1',  tools=[{\"type\": \"web_search_preview\"}],input=\"What is today's date\")\n",
        "print(Response.output_text)\n",
        "print(Response)\n",
        "\n",
        "#print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2Gy452CueL9",
        "outputId": "1062e22b-bb6e-46d6-a74e-2e5b9f0db719"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apr 29, 2025, 6:09:19 AM \n",
            "Response(id='resp_6810a54c962881918175460231c67c5e0dceb501030cc726', created_at=1745921356.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-2025-04-14', object='response', output=[ResponseFunctionWebSearch(id='ws_6810a54d37cc81918231285022163d570dceb501030cc726', status='completed', type='web_search_call'), ResponseOutputMessage(id='msg_6810a54f0124819180a8e980238385970dceb501030cc726', content=[ResponseOutputText(annotations=[], text='Apr 29, 2025, 6:09:19\\u202fAM ', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[WebSearchTool(type='web_search_preview', search_context_size='medium', user_location=UserLocation(type='approximate', city=None, country='US', region=None, timezone=None))], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=305, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=32, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=337), user=None, store=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "response = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts,\n",
        "                                        max_tokens=3,   temperature=0.8,  logprobs=True,     seed=42,         top_logprobs=7,        stream=True)\n",
        "predictions = []\n",
        "for chunk in response:\n",
        "            if chunk.choices[0].delta.content:\n",
        "                token = chunk.choices[0].delta.content\n",
        "                logprobs = chunk.choices[0].logprobs.content[0].top_logprobs\n",
        "                logprob_dict = {item.token: item.logprob for item in logprobs}\n",
        "\n",
        "                # Get top predicted token and probability\n",
        "                top_token = token\n",
        "                top_prob = logprob_dict[token]\n",
        "\n",
        "                # Get alternative predictions\n",
        "                alternatives = []\n",
        "                for alt_token, alt_prob in logprob_dict.items():\n",
        "                    if alt_token != token:\n",
        "                        alternatives.append((alt_token, math.exp(alt_prob)))\n",
        "                alternatives.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                prediction = {'token': top_token, 'probability': math.exp(top_prob),'alternatives': alternatives[:3]}\n",
        "                predictions.append(prediction)\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "N9-ExQJG8aIF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}