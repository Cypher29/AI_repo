{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiKW+UbAgeIPFuTBQqMWiy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cypher29/AI_repo/blob/main/tokenizing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "7dCWrcV9wk3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel"
      ],
      "metadata": {
        "id": "QlEACyLpv8X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGdPwnoPCJrT"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import pipeline\n",
        "#import anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HUGGINGFACE_KEY')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "CaEuVaCVDgmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoModel.from_pretrained('google-bert/bert-base-uncased')"
      ],
      "metadata": {
        "id": "eqdsjcs4xuT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_9xpPBRB2AA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(config)"
      ],
      "metadata": {
        "id": "FLOs0Ag6x4uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wjnKIFAwj2OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is sentiment analysis model"
      ],
      "metadata": {
        "id": "-GaXguRBFdaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased-finetuned-sst-2-english', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "WAepaFSyD7Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "qQ_81rDEkjBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer)"
      ],
      "metadata": {
        "id": "4OdFUN_mkDnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "bMdL7IagmB5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, Explain about software development life cycle\"\n",
        "tokens = tokenizer.encode(text)\n",
        "vocabsize = tokenizer.vocab_size\n",
        "print(\"Vocab size : \" ,vocabsize)\n",
        "tokens"
      ],
      "metadata": {
        "id": "s9HQrJJMEU2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "JWzxPub_Ee5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokens)"
      ],
      "metadata": {
        "id": "oW4-suVSEkZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.all_special_tokens"
      ],
      "metadata": {
        "id": "Rcx1dbd5lMtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(tokens)"
      ],
      "metadata": {
        "id": "NApVc6TpEoom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_added_vocab()"
      ],
      "metadata": {
        "id": "PvwdQwoTEucl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\""
      ],
      "metadata": {
        "id": "sDbQV0ZZFawn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "tokens = phi3_tokenizer.encode(text)\n",
        "print(phi3_tokenizer.batch_decode(tokens))"
      ],
      "metadata": {
        "id": "n7N02WR-FlZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.all_special_tokens\n",
        "tokenizer.get_vocab()\n",
        "tokenizer.verbose"
      ],
      "metadata": {
        "id": "RGOP6f4xX8tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "len(inputs)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "print(logits)\n",
        "predicted_class_id = logits.argmax().item()\n",
        "print(predicted_class_id)\n",
        "model.config.id2label[predicted_class_id]"
      ],
      "metadata": {
        "id": "svLmVjDolatV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qlTIiUSmt7Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWnEfXmFeHVK"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_KEY')\n",
        "MODEL = 'gpt-4o-mini'\n",
        "openai = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant that is great at helping with logical answers. Always be accurate. If you don't know the answer, say so.\"\n",
        "#user_prompt = \"Plucking apples from apple tree in apestreat. How many p's are there in above sentense\"\n",
        "user_prompt = \"any difference between 539317 and 571393\"\n",
        "prompts = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]"
      ],
      "metadata": {
        "id": "CHKEyhqRuHqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gpt-3.5-turbo\n",
        "#gpt-4.1\n",
        "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "UxNDRorhwgDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
        "Response = openai.responses.create(model='gpt-4.1',  tools=[{\"type\": \"web_search_preview\"}],input=\"What is today's date\")\n",
        "print(Response.output_text)\n",
        "\n",
        "#print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "M2Gy452CueL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "response = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts,\n",
        "                                            max_tokens=3,\n",
        "            temperature=0.8,\n",
        "            logprobs=True,\n",
        "            seed=42,\n",
        "            top_logprobs=7,\n",
        "            stream=True)\n",
        "predictions = []\n",
        "for chunk in response:\n",
        "            if chunk.choices[0].delta.content:\n",
        "                token = chunk.choices[0].delta.content\n",
        "                logprobs = chunk.choices[0].logprobs.content[0].top_logprobs\n",
        "                logprob_dict = {item.token: item.logprob for item in logprobs}\n",
        "\n",
        "                # Get top predicted token and probability\n",
        "                top_token = token\n",
        "                top_prob = logprob_dict[token]\n",
        "\n",
        "                # Get alternative predictions\n",
        "                alternatives = []\n",
        "                for alt_token, alt_prob in logprob_dict.items():\n",
        "                    if alt_token != token:\n",
        "                        alternatives.append((alt_token, math.exp(alt_prob)))\n",
        "                alternatives.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                prediction = {'token': top_token, 'probability': math.exp(top_prob),'alternatives': alternatives[:2]}\n",
        "                predictions.append(prediction)\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "N9-ExQJG8aIF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}