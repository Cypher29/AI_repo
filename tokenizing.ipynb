{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOzfjWEtrFHMvVKJoq8q4hS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cypher29/AI_repo/blob/main/tokenizing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "7dCWrcV9wk3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel"
      ],
      "metadata": {
        "id": "QlEACyLpv8X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGdPwnoPCJrT"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import pipeline\n",
        "#import anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HUGGINGFACE_KEY')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "CaEuVaCVDgmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoModel.from_pretrained('google-bert/bert-base-uncased')"
      ],
      "metadata": {
        "id": "eqdsjcs4xuT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_9xpPBRB2AA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(config)"
      ],
      "metadata": {
        "id": "FLOs0Ag6x4uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "QWEN2_MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
        "STARCODER2_MODEL_NAME = \"bigcode/starcoder2-3b\""
      ],
      "metadata": {
        "id": "kwTuF6awGLHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, Explain about software development life cycle\""
      ],
      "metadata": {
        "id": "W9Z3ND0AQ_fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text1 = \"Hello, Explain about software development life cycle\"\n",
        "#tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased', trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased-finetuned-sst-2-english', trust_remote_code=True)\n",
        "tokens = tokenizer.encode(text)\n",
        "print(tokens)\n",
        "print(tokenizer.batch_decode(tokens));"
      ],
      "metadata": {
        "id": "WAepaFSyD7Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text1 = \"Hello, Explain about software development life cycle\"\n",
        "star_tokenizer = AutoTokenizer.from_pretrained(STARCODER2_MODEL_NAME, trust_remote_code=True)\n",
        "star_tokens = star_tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "5RkEVUZYQbVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text2 = \"Hello, Explain about software development life cycle\"\n",
        "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME, trust_remote_code=True)\n",
        "phi3_tokens = phi3_tokenizer.encode(text)\n",
        "\n"
      ],
      "metadata": {
        "id": "90Oax49OGQpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text3 = \"Hello, Explain about software development life cycle\"\n",
        "qwen2_tokenizer = AutoTokenizer.from_pretrained(QWEN2_MODEL_NAME)\n",
        "qwen2_tokens = qwen2_tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "n9u33mFrRgby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(star_tokens)\n",
        "print(star_tokenizer.batch_decode(star_tokens));\n",
        "print()\n",
        "print(phi3_tokens)\n",
        "print(phi3_tokenizer.batch_decode(phi3_tokens));\n",
        "print()\n",
        "print(qwen2_tokens)\n",
        "print(qwen2_tokenizer.batch_decode(qwen2_tokens));"
      ],
      "metadata": {
        "id": "R4gxqjF_GRiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wjnKIFAwj2OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is sentiment analysis model"
      ],
      "metadata": {
        "id": "-GaXguRBFdaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer)"
      ],
      "metadata": {
        "id": "4OdFUN_mkDnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "bMdL7IagmB5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, Explain about software development life cycle\"\n",
        "tokens = tokenizer.encode(text)\n",
        "vocabsize = tokenizer.vocab_size\n",
        "print(\"Vocab size : \" ,vocabsize)\n",
        "tokens"
      ],
      "metadata": {
        "id": "s9HQrJJMEU2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "JWzxPub_Ee5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokens)"
      ],
      "metadata": {
        "id": "oW4-suVSEkZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.all_special_tokens"
      ],
      "metadata": {
        "id": "Rcx1dbd5lMtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(tokens)"
      ],
      "metadata": {
        "id": "NApVc6TpEoom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_added_vocab()"
      ],
      "metadata": {
        "id": "PvwdQwoTEucl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\""
      ],
      "metadata": {
        "id": "sDbQV0ZZFawn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "tokens = phi3_tokenizer.encode(text)\n",
        "print(phi3_tokenizer.batch_decode(tokens))"
      ],
      "metadata": {
        "id": "n7N02WR-FlZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.all_special_tokens\n",
        "tokenizer.get_vocab()\n"
      ],
      "metadata": {
        "id": "RGOP6f4xX8tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "inputs = tokenizer(\"My bike has trouble and I feel good as I don't need to got to market \", return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "len(inputs)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "print(logits)\n",
        "predicted_class_id = logits.argmax().item()\n",
        "print(predicted_class_id)\n",
        "model.config.id2label[predicted_class_id]"
      ],
      "metadata": {
        "id": "svLmVjDolatV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qlTIiUSmt7Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWnEfXmFeHVK"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_KEY')\n",
        "MODEL = 'gpt-4o-mini'\n",
        "openai = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant that is great at completing sentense. Always be accurate. If you don't know the answer, say so.\"\n",
        "#user_prompt = \"Plucking apples from apple tree in apestreat. How many p's are there in above sentense\"\n",
        "user_prompt = \"food ball is a game\"\n",
        "prompts = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]"
      ],
      "metadata": {
        "id": "CHKEyhqRuHqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gpt-3.5-turbo\n",
        "#gpt-4.1\n",
        "completion = openai.chat.completions.create(model='gpt-4.1', messages=prompts, temperature=0.8)\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "UxNDRorhwgDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
        "Response = openai.responses.create(model='gpt-4.1',  tools=[{\"type\": \"web_search_preview\"}],input=\"What is today's date\")\n",
        "print(Response.output_text)\n",
        "print(Response)\n",
        "\n",
        "#print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "M2Gy452CueL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "response = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts,\n",
        "                                        max_tokens=3,   temperature=0.8,  logprobs=True,     seed=42,         top_logprobs=7,        stream=True)\n",
        "predictions = []\n",
        "for chunk in response:\n",
        "            if chunk.choices[0].delta.content:\n",
        "                token = chunk.choices[0].delta.content\n",
        "                logprobs = chunk.choices[0].logprobs.content[0].top_logprobs\n",
        "                logprob_dict = {item.token: item.logprob for item in logprobs}\n",
        "\n",
        "                # Get top predicted token and probability\n",
        "                top_token = token\n",
        "                top_prob = logprob_dict[token]\n",
        "\n",
        "                # Get alternative predictions\n",
        "                alternatives = []\n",
        "                for alt_token, alt_prob in logprob_dict.items():\n",
        "                    if alt_token != token:\n",
        "                        alternatives.append((alt_token, math.exp(alt_prob)))\n",
        "                alternatives.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                prediction = {'token': top_token, 'probability': math.exp(top_prob),'alternatives': alternatives[:3]}\n",
        "                predictions.append(prediction)\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "N9-ExQJG8aIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization"
      ],
      "metadata": {
        "id": "ok9EE7wx16kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate\n",
        "!pip install -q requests  bitsandbytes sentencepiece accelerate"
      ],
      "metadata": {
        "id": "R3DoGKUG1u7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  AutoTokenizer,AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "#import gc"
      ],
      "metadata": {
        "id": "6xE-DKw93VwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LLAMA = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct\"\n",
        "LLAMA = \"bigcode/starcoder2-3b\""
      ],
      "metadata": {
        "id": "Fnvu8e7C2S49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a good helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a proverb\"}\n",
        "  ]"
      ],
      "metadata": {
        "id": "KgxCLBJIT5Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "CZ6Sog6P2eCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "V8pO2Gsf2i1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)"
      ],
      "metadata": {
        "id": "21Qe7Me82nL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_model_mem = model.get_memory_footprint() / 1e6\n",
        "print(f\"Memory : {total_model_mem:,.1f} MB\")"
      ],
      "metadata": {
        "id": "U7qASe8g2pq_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}