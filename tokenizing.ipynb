{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuyrOQIjEoEC9UJI6iWbe9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cypher29/AI_repo/blob/main/tokenizing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "7dCWrcV9wk3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel"
      ],
      "metadata": {
        "id": "QlEACyLpv8X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGdPwnoPCJrT"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import pipeline\n",
        "import anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HUGGINGFACE_KEY')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "CaEuVaCVDgmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoModel.from_pretrained('microsoft/MAI-DS-R1')"
      ],
      "metadata": {
        "id": "eqdsjcs4xuT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_9xpPBRB2AA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(config)"
      ],
      "metadata": {
        "id": "FLOs0Ag6x4uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wjnKIFAwj2OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is sentiment analysis model"
      ],
      "metadata": {
        "id": "-GaXguRBFdaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased-finetuned-sst-2-english', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "WAepaFSyD7Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "qQ_81rDEkjBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer)"
      ],
      "metadata": {
        "id": "4OdFUN_mkDnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "bMdL7IagmB5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "tokens = tokenizer.encode(text)\n",
        "vocabsize = tokenizer.vocab_size\n",
        "print(\"Vocab size : \" ,vocabsize)\n",
        "total_parameters = sum(p.numel() for p in Params.parameters())\n",
        "print(\"Total parameters : \" ,total_parameters)\n",
        "tokens"
      ],
      "metadata": {
        "id": "s9HQrJJMEU2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "JWzxPub_Ee5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokens)"
      ],
      "metadata": {
        "id": "oW4-suVSEkZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(tokens)"
      ],
      "metadata": {
        "id": "NApVc6TpEoom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_added_vocab()"
      ],
      "metadata": {
        "id": "PvwdQwoTEucl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\""
      ],
      "metadata": {
        "id": "sDbQV0ZZFawn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "tokens = phi3_tokenizer.encode(text)\n",
        "print(phi3_tokenizer.batch_decode(tokens))"
      ],
      "metadata": {
        "id": "n7N02WR-FlZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.all_special_tokens"
      ],
      "metadata": {
        "id": "RGOP6f4xX8tc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}